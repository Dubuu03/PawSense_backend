"""
Detection service for running TensorFlow Lite inference
"""

import io
import os
import numpy as np
from typing import List, Dict, Any
from PIL import Image
from fastapi import HTTPException, UploadFile

try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    # Fallback to TensorFlow Lite from full TensorFlow
    import tensorflow as tf
    tflite = tf.lite

from app.models.schemas import Detection, DetectionResponse, ModelInfo
from app.services.model_service import model_service
from app.utils.config import config


class DetectionService:
    """Service for handling image detection operations"""
    
    @staticmethod
    def validate_image(file: UploadFile) -> None:
        """
        Validate uploaded image file
        Args:
            file: Uploaded file object
        """
        print(f"ðŸ” Validating file: {file.filename}")
        print(f"ðŸ“„ Content-Type received: '{file.content_type}'")
        print(f"ðŸ“‹ Allowed types from config: {config.ALLOWED_FILE_TYPES}")
        
        # Normalize MIME type
        content_type = (file.content_type or '').lower().strip()
        allowed_types = [t.lower().strip() for t in config.ALLOWED_FILE_TYPES]
        
        print(f"ðŸ“„ Normalized content-type: '{content_type}'")
        print(f"ðŸ“‹ Normalized allowed types: {allowed_types}")
        
        # Accept common variants
        extra_types = ['image/pjpeg', 'image/x-png', 'image/x-bmp', 'image/gif', 'image/webp']
        allowed_types += extra_types
        
        # Check file type
        is_valid_type = content_type in allowed_types
        print(f"âœ… Type validation result: {is_valid_type}")
        
        if not is_valid_type:
            # Fallback: check file extension
            import os
            ext = os.path.splitext(file.filename or '')[1].lower()
            allowed_exts = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif', '.webp']
            is_valid_ext = ext in allowed_exts
            
            print(f"ðŸ“ File extension: '{ext}'")
            print(f"ðŸ“‹ Allowed extensions: {allowed_exts}")
            print(f"âœ… Extension validation result: {is_valid_ext}")
            
            if not is_valid_ext:
                print(f"âŒ File validation failed for: {file.filename}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid file type. Allowed types: {config.ALLOWED_FILE_TYPES}"
                )
            else:
                print(f"âœ… File accepted via extension fallback")
        else:
            print(f"âœ… File accepted via content-type")
            
        # Check file size
        if hasattr(file, 'size') and file.size and file.size > config.MAX_FILE_SIZE:
            max_size_mb = config.MAX_FILE_SIZE / (1024 * 1024)
            raise HTTPException(status_code=400, detail=f"File size too large. Maximum {max_size_mb}MB allowed.")
    
    @staticmethod
    def run_inference(interpreter, image: Image.Image, labels: Dict[int, str]) -> List[Detection]:
        """
        Run TensorFlow Lite inference on an image and format results
        
        Args:
            interpreter: Loaded TensorFlow Lite interpreter
            image: PIL Image object
            labels: Dictionary mapping class IDs to label names
            
        Returns:
            List of Detection objects
        """
        try:
            # Get input and output details
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            
            # Get input shape
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            
            # Preprocess image
            image_resized = image.resize((width, height))
            image_rgb = image_resized.convert('RGB')
            input_data = np.array(image_rgb, dtype=np.float32)
            
            # Normalize to [0, 1] if needed
            if input_details[0]['dtype'] == np.float32:
                input_data = input_data / 255.0
                
            # Add batch dimension
            input_data = np.expand_dims(input_data, axis=0)
            
            # Set input tensor
            interpreter.set_tensor(input_details[0]['index'], input_data)
            
            # Run inference
            interpreter.invoke()
            
            # Get output tensors
            # Assuming YOLO output format: [batch, detections, 6] where 6 = [x, y, w, h, conf, class]
            output_data = interpreter.get_tensor(output_details[0]['index'])
            
            detections = []
            
            # Process detections
            for detection in output_data[0]:  # Remove batch dimension
                if len(detection) >= 6:
                    x_center, y_center, w, h, confidence, class_id = detection[:6]
                    
                    # Skip low confidence detections
                    if confidence < 0.5:
                        continue
                    
                    # Convert from center format to corner format
                    x1 = (x_center - w/2) * image.width
                    y1 = (y_center - h/2) * image.height
                    x2 = (x_center + w/2) * image.width
                    y2 = (y_center + h/2) * image.height
                    
                    # Get label name
                    class_id = int(class_id)
                    label = labels.get(class_id, f"Unknown_{class_id}")
                    
                    detection_obj = Detection(
                        class_id=class_id,
                        label=label,
                        confidence=round(float(confidence), 4),
                        bbox=[round(x1, 2), round(y1, 2), round(x2, 2), round(y2, 2)]
                    )
                    
                    detections.append(detection_obj)
            
            return detections
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error during inference: {str(e)}")
    
    @staticmethod
    async def process_detection(model_type: str, file: UploadFile) -> DetectionResponse:
        """
        Process image detection for a specific model type
        
        Args:
            model_type: Either 'cats' or 'dogs'
            file: Uploaded image file
            
        Returns:
            DetectionResponse object
        """
        # Validate image
        DetectionService.validate_image(file)
        
        try:
            # Read and process image
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Get model resources
            model = model_service.get_model(model_type)
            labels = model_service.get_labels(model_type)
            metadata = model_service.get_metadata(model_type)
            
            # Run inference
            detections = DetectionService.run_inference(model, image, labels)
            
            # Prepare response
            response = DetectionResponse(
                filename=file.filename or "unknown.jpg",
                model_info=ModelInfo(**metadata),
                detections=detections,
                total_detections=len(detections)
            )
            
            return response
        
        except Exception as e:
            if isinstance(e, HTTPException):
                raise e
            else:
                raise HTTPException(status_code=500, detail=f"Error processing image: {str(e)}")


# Global detection service instance
detection_service = DetectionService()